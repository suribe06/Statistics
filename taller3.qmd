---
title: Taller 3. Eduardo Avendano y Santiago Uribe
jupyter: python3
---



```{python}
import pandas as pd
import statsmodels.formula.api as smf
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
```

```{python}
df = pd.read_csv('baseball.csv')
df = df.drop('name', axis=1)
dim_df = df.shape
print(f"The dataset consists of {dim_df[0]} rows and {dim_df[1]} columns")
```

Data types of the columns:

```{python}
df.dtypes
```

Let’s check how much missing data there is by columns

```{python}
# Missing values by column
df.isnull().sum()
```

Descriptive Statistics

```{python}
df.describe()
```

Histograms

```{python}
col_titles = ['Salary', 'Avg. Batting', 'On-Base Pct.', 'Runs', 'Hits', 'Doubles', 'Triples', 'Homeruns', 
               'Runs Batted In', 'Walks', 'Strikeouts', 'Stolen Bases', 'Errors']
fig, axs = plt.subplots(3, 5, figsize=(12,6))
axs = axs.ravel()
for i, col in enumerate(df.columns):
    axs[i].hist(df[col], edgecolor='black')
    axs[i].set_title(col_titles[i])
    
plt.tight_layout()
plt.show()
```

Boxplots

```{python}
import matplotlib.patches as mpatches

fliers = dict(markerfacecolor='m', marker='D') #atypical data
mean_ = dict(markerfacecolor='green', marker='D')
mean_artist = mpatches.Patch(facecolor='green', label='Mean')
ad_artist = mpatches.Patch(color='m', label='Atypical Data')

fig, axs = plt.subplots(3, 5, figsize=(12,6))
axs = axs.ravel()
for i, col in enumerate(df.columns):
  bp = axs[i].boxplot(df[col], vert=False, flierprops=fliers, showmeans=True, meanprops=mean_)
  axs[i].legend(handles=[mean_artist, ad_artist], loc='upper left', fontsize=8)
  axs[i].set_title(col_titles[i])

plt.tight_layout()
plt.show()
```

The analysis conducted shows that both in the histograms and box plots, the batting average and on-base percentage variables are skewed to the left, while the triples, errors, and stolen bases variables are skewed to the right, and these same three variables have the most atypical data. It can also be observed that the batting average and on-base percentage variables have a small standard deviation due to the size of the whiskers. Finally, for most variables, the median and mean have similar values.

Correlation

```{python}
fig, ax = plt.subplots(figsize=(10, 10))
corr_matrix = df.corr()
corr_matrix = corr_matrix.rename(columns=dict(zip(corr_matrix.columns, col_titles)))
corr_matrix = corr_matrix.rename(index=dict(zip(corr_matrix.index, col_titles)))
sns.heatmap(corr_matrix, square=True, annot=True, ax=ax)
plt.show()
```

The correlation matrix shows that several variables have high correlation coefficients (such as 0.89, 0.84, and 0.8). Therefore, it is important to exercise caution when dealing with variables that have a high degree of correlation, as they can have a significant impact on the model results.

# Linear Regression Model

```{python}
from scipy.stats import shapiro
from statsmodels.stats.diagnostic import het_breuschpagan

def checkModelAssumptions(model):
    residuos = model.resid
    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 6))
    #QQ plot
    sm.qqplot(residuos, line='45', ax=axs[0,0])
    axs[0,0].set_title('Residuals Q-Q plot')
    #Residuals Histogram
    axs[0,1].hist(residuos, bins=30, edgecolor='black')
    axs[0,1].set_xlabel('Residuos')
    axs[0,1].set_ylabel('Frecuencia')
    axs[0,1].set_title('Residuals Histogram')
    #Reiduals Boxplot
    axs[1,0].boxplot(residuos)
    axs[1,0].set_ylabel('Residuals')
    axs[1,0].set_title('Residuals Boxplot')
    #Residuals
    axs[1,1].scatter(model.fittedvalues, model.resid_pearson)
    axs[1,1].plot([min(model.fittedvalues), max(model.fittedvalues)], [0, 0], 'k--', lw=2)
    axs[1,1].set_xlabel('Fitted Values')
    axs[1,1].set_ylabel('Residuals')
    axs[1,1].set_title(' Standarized Residuals')
    plt.tight_layout()
    plt.show()

    #Normality test
    print('================= Normality test =================')
    stat, p = shapiro(residuos)
    print('Estadística de prueba:', stat)
    print('Valor p:', p)
    if p <= 0.05: print("Se rechaza H0")
    else: print("No se rechaza H0")
    #Homoscedasticity test
    print('================= Homoscedasticity test =================')
    lm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(residuos, model.model.exog)
    print('Lagrange multiplier statistic:', lm)
    print('p-value', lm_pvalue)
    print('F value:', fvalue)
    print('F p-value', f_pvalue)
    if lm_pvalue <= 0.05: print("Se rechaza H0")
    else: print("No se rechaza H0")
```

Dataset split into train and test

```{python}
# Agregando una columna de unos para el intercepto
df = sm.add_constant(df)
```

```{python}
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop('salary', axis=1), 
                                                    df['salary'], test_size=0.2, random_state=123)
```

```{python}
# Fitting the regression model
complete_model = sm.OLS(y_train, X_train).fit()
print(complete_model.summary())
```

```{python}
print('Coefficients LR')
print(complete_model.params)
```

Significant variables for the model

```{python}
print('p-values variables')
print(complete_model.pvalues)
```

Metrics to check the model

```{python}
from sklearn.metrics import mean_squared_error

y_test_pred = complete_model.predict(X_test)
mse = mean_squared_error(y_test, y_test_pred)
print(f'MSE for the LR Model: {mse}')
```

Check model assumptions:

```{python}
checkModelAssumptions(complete_model)
```

The linear regression model of all variables has an adjusted R-squared of 0.445, suggesting that approximately 44.5% of the variability of the response variable (salary) can be explained by the predictor variables included in the model.

Some of the regression coefficients have high p-values, suggesting that they are not statistically significant. For example, the coefficient for "hits" has a p-value of 0.936797, suggesting that it is not a significant variable for predicting salary. In addition, the variable "RBI" has a marginally significant p-value (0.057780), suggesting that it may be an important variable, but further analysis is needed to confirm its significance.

Regarding the MSE, the obtained value indicates that there is high variance between the values predicted by the model and the actual values of the target variable. In other words, the model does not fit the data well and underperforms in the prediction task. This may occur because there are variables that are highly correlated, as evidenced by values of 0.89 and 0.84.

With respect to check the model assumptions since the null hypothesis is rejected in Shapiro Wilk test, the residuals are not from a normal distribution. It can also be verified that the QQ plot does not conform to a normal distribution. In homoscedasticity test, since H0 is not rejected, it means that errors have a constant variance in all observations and, therefore, there is no heteroscedasticity. Furthermore, it can be seen that the residuals are evenly and randomly distributed around zero.

### Modelos Propuestos:

Selection of the best variables using a greedy approach. At each stage, this estimator chooses the best feature to add based on the cross-validation score of an estimator.

```{python}
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

df_copy = df.copy()
df_copy = df_copy.drop('const', axis=1)
X_train, X_test, y_train, y_test = train_test_split(df_copy.drop('salary', axis=1), 
                                                    df_copy['salary'], test_size=0.2, random_state=123)

# Model
model = LinearRegression().fit(X_train, y_train)
sfs = SequentialFeatureSelector(model, direction='forward', n_features_to_select=3)
sfs.fit(X_train, y_train)

# Variables seleccionadas
selected_features = X_train.columns[sfs.get_support()]
print("Variables seleccionadas:", selected_features)

# Entrenar el modelo con las variables seleccionadas
X_selected = X_train.iloc[:, sfs.get_support()]
model_final = LinearRegression().fit(X_selected, y_train)
coef_dict = dict(zip(X_selected.columns, model_final.coef_))
coef_dict['intercept'] = model_final.intercept_
print(coef_dict)
y_pred_selected = model_final.predict(X_test.iloc[:, sfs.get_support()])
mse_selected = mean_squared_error(y_test, y_pred_selected)
print(f"MSE del modelo con variables seleccionadas: {mse_selected}")
```

```{python}
residuos = y_test - y_pred_selected
#Normality test
print('================= Normality test =================')
stat, p = shapiro(residuos)
print('Estadística de prueba:', stat)
print('Valor p:', p)
if p <= 0.05: print("Se rechaza H0")
else: print("No se rechaza H0")
#Homoscedasticity test
print('================= Homoscedasticity test =================')
lm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(residuos, X_test)
print('Lagrange multiplier statistic:', lm)
print('p-value', lm_pvalue)
print('F value:', fvalue)
print('F p-value', f_pvalue)
if lm_pvalue <= 0.05: print("Se rechaza H0")
else: print("No se rechaza H0")
```

For both tests (homoscedasticity and normality) H0 is not rejected, so the residuals come from a normal distribution and have constant variance.

Model removing pairs of variables with high correlation, hits-runs (0.89) 

```{python}
df_copy = df.copy()
df_copy = df_copy.drop(['hits', 'runs'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(df_copy.drop('salary', axis=1), 
                                                    df_copy['salary'], test_size=0.2, random_state=123)
model2 = sm.OLS(y_train, X_train).fit()
print(model2.summary())
```

```{python}
y_test_pred = model2.predict(X_test)
mse = mean_squared_error(y_test, y_test_pred)
print(f'MSE for the LR Model: {mse}')
```

```{python}
checkModelAssumptions(model2)
```

The linear regression model has an adjusted R-squared of 0.454, suggesting that approximately 45.4% of the variability of the response variable (salary) can be explained by the predictor variables included in the model.

The MSE is slightly higher than the full model.

Since the null hypothesis is rejected in Shapiro Wilk test, the residuals are not from a normal distribution. In homoscedasticity test, since H0 is not rejected, it means that errors have a constant variance in all observations and, therefore, there is no heteroscedasticity. Furthermore, it can be seen that the residuals are evenly and randomly distributed around zero.

Model removing pairs of variables with high correlation, hits-runs (0.89) and doubles-homeruns (0.84)

```{python}
df_copy = df.copy()
df_copy = df_copy.drop(['hits', 'runs', 'dooubles', 'homeruns'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(df_copy.drop('salary', axis=1), 
                                                    df_copy['salary'], test_size=0.2, random_state=123)
model3 = sm.OLS(y_train, X_train).fit()
print(model3.summary())
```

```{python}
y_test_pred = model3.predict(X_test)
mse = mean_squared_error(y_test, y_test_pred)
print(f'MSE for the LR Model: {mse}')
```

```{python}
checkModelAssumptions(model3)
```

The linear regression model has an adjusted R-squared of 0.458, suggesting that approximately 45.8% of the variability of the response variable (salary) can be explained by the predictor variables included in the model.

In terms of the MSE, it has a lower MSE than the full model, which indicates that this model fits the data better, although the MSE is still high.

Since the null hypothesis is rejected in Shapiro Wilk test, the residuals are not from a normal distribution. In homoscedasticity test, since H0 is not rejected, it means that errors have a constant variance in all observations and, therefore, there is no heteroscedasticity. Furthermore, it can be seen that the residuals are evenly and randomly distributed around zero.

Model removing pairs of variables with high correlations, hits-runs (0.89) and OBP-batting (0.8)

```{python}
df_copy = df.copy()
df_copy = df_copy.drop(['hits', 'runs', 'OBP', 'batting'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(df_copy.drop('salary', axis=1), 
                                                    df_copy['salary'], test_size=0.2, random_state=123)
model4 = sm.OLS(y_train, X_train).fit()
print(model4.summary())
```

```{python}
y_test_pred = model4.predict(X_test)
mse = mean_squared_error(y_test, y_test_pred)
print(f'MSE for the LR Model: {mse}')
```

```{python}
checkModelAssumptions(model4)
```

The linear regression model has an adjusted R-squared of 0.463, suggesting that approximately 46.3% of the variability of the response variable (salary) can be explained by the predictor variables included in the model.

The MSE is slightly higher than the full model.

Since the null hypothesis is rejected in Shapiro Wilk test, the residuals are not from a normal distribution. In homoscedasticity test, since H0 is rejected, it means that there is sufficient evidence to conclude that the variance of the errors is not constant and, therefore, that there is heteroscedasticity in the model.

Model removing the variables with the highest p-value from the complete model:

```{python}
df_copy = df.copy()
df_copy = df_copy.drop(['hits', 'walks', 'OBP', 'batting'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(df_copy.drop('salary', axis=1), 
                                                    df_copy['salary'], test_size=0.2, random_state=123)
model5 = sm.OLS(y_train, X_train).fit()
print(model5.summary())
```

```{python}
y_test_pred = model5.predict(X_test)
mse = mean_squared_error(y_test, y_test_pred)
print(f'MSE for the LR Model: {mse}')
```

```{python}
checkModelAssumptions(model5)
```

The linear regression model has an adjusted R-squared of 0.467, suggesting that approximately 46.7% of the variability of the response variable (salary) can be explained by the predictor variables included in the model.

In terms of the MSE, it has a lower MSE than the full model, which indicates that this model fits the data better, although the MSE is still high.

Since the null hypothesis is rejected in Shapiro Wilk test, the residuals are not from a normal distribution. In homoscedasticity test, since H0 is not rejected, it means that errors have a constant variance in all observations and, therefore, there is no heteroscedasticity. Furthermore, it can be seen that the residuals are evenly and randomly distributed around zero.

Model considering a PCA analysis:

```{python}
from sklearn.decomposition import PCA

pca = PCA(n_components=13)
pca.fit(df.drop(['salary'], axis=1)) 
X_pca = pca.fit_transform(df.drop(['salary'], axis=1))

expl = pca.explained_variance_ratio_
print('suma:',sum(expl[0:5]))

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.xticks(np.arange(0, df.shape[1]-1, 1.0))

plt.ylabel('cumulative explained variance')
plt.grid()
plt.show()
```

```{python}
X_pca_df = pd.DataFrame(X_pca[:, :4], columns=['PC1', 'PC2', 'PC3', 'PC4'])
X_pca_df['salary'] = df['salary']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_pca_df.drop('salary', axis=1), X_pca_df['salary'], test_size=0.2, random_state=123)
model_pca = sm.OLS(y_train, X_train).fit()
print(model_pca.summary())
```

```{python}
y_test_pred = model_pca.predict(X_test)
mse = mean_squared_error(y_test, y_test_pred)
print(f'MSE for the LR Model: {mse}')
```

```{python}
checkModelAssumptions(model_pca)
```

The linear regression model has an adjusted R-squared of 0.08, suggesting that approximately 8% of the variability of the response variable (salary) can be explained by the predictor variables included in the model.

In terms of MSE it is the worst of all, we can see how it shoots up abruptly. The model does not fit the data

Since the null hypothesis is rejected in Shapiro Wilk test, the residuals are not from a normal distribution. In homoscedasticity test, since H0 is not rejected, it means that errors have a constant variance in all observations and, therefore, there is no heteroscedasticity. Although the residuals have a random distribution, the residuals have a positive trend relative to the fitted values, this could indicate that the model is not capturing a nonlinear relationship between the predictor variables and the response variable

### RESUMEN DE RESULTADOS

| Modelo                          | MSE          |
|---------------------------------|--------------|
| Modelo 2 elimnado correlaciones | 451,816.87   |
| Modelo sin p-values grandes     | 465,509.71   |
| Modelo Completo                 | 472,509.23   |
| Modelo 1 elimnado correlaciones | 488,929.62   |
| Modelo 3 elimnado correlaciones | 496,001.74   |
| Modelo Greedy                   | 500,367.14   |
| Modelo PCA                      | 6,201,010.12 |

# Punto 2

```{python}
df2 = pd.read_csv('fertility.csv')
df2 = df2.drop(['voc_train', 'german'], axis=1)
dim_df = df2.shape
print(f"The dataset consists of {dim_df[0]} rows and {dim_df[1]} columns")
```

```{python}
df2.isnull().sum()
```

Convert cathegorical data to numerical

```{python}
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
categorical_vars = ['university', 'religion', 'rural']
for var in categorical_vars:
    df2[var] = le.fit_transform(df2[var])
df2
```

```{python}
X_train = df2.drop('children', axis=1)
y_train = df2['children']
```

### Poisson Model

```{python}
model_poisson = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit()
print(model_poisson.summary())
```

As shown in the table, all variables except "religion" and "age_marriage" have p-values less than 0.05, indicating that they are statistically significant in the model. However, since the p-values of "religion" and "age_marriage" are 0.119 > 0.05 and 0.066 > 0.05, respectively, it cannot be concluded that these variables have a significant effect on the model.

The negative coefficient of "university", as a binary categorical variable (yes/no), suggests that women with this characteristic (i.e., a "yes") have fewer children.

On the other hand, the positive coefficient of "rural" suggests that women with this characteristic have more children.

```{python}
sm.qqplot(model_poisson.resid_response, line='s')
plt.title('QQ-plot de los residuos')
plt.show()
```

```{python}
#Normality test
print('================= Normality test =================')
stat, p = shapiro(model_poisson.resid_response)
print('Estadística de prueba:', stat)
print('Valor p:', p)
if p <= 0.05: print("Se rechaza H0")
else: print("No se rechaza H0")
#Homoscedasticity test
print('================= Homoscedasticity test =================')
lm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(model_poisson.resid_response, model_poisson.model.exog)
print('Lagrange multiplier statistic:', lm)
print('p-value', lm_pvalue)
print('F value:', fvalue)
print('F p-value', f_pvalue)
if lm_pvalue <= 0.05: print("Se rechaza H0")
else: print("No se rechaza H0")
```

### Negative Binomial Model

Si la razón entre el Chi-cuadrado y los grados de libertad es mayor a 1, entonces existe sobredispersión.

En este caso, el Pearson Chi-cuadrado es de 1300, con 1237 grados de libertad, lo que resulta en una razón de 1.0509. Como esta razón es mayor a 1, se puede concluir que los datos presentan sobredispersión.

```{python}
model_nb = sm.GLM(y_train, X_train, family=sm.families.NegativeBinomial()).fit()
print(model_nb.summary())
```

```{python}
sm.qqplot(model_nb.resid_response, line='s')
plt.title('QQ-plot')
plt.show()
```

```{python}
#Normality test
print('================= Normality test =================')
stat, p = shapiro(model_nb.resid_response)
print('Estadística de prueba:', stat)
print('Valor p:', p)
if p <= 0.05: print("Se rechaza H0")
else: print("No se rechaza H0")
#Homoscedasticity test
print('================= Homoscedasticity test =================')
lm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(model_nb.resid_response, model_nb.model.exog)
print('Lagrange multiplier statistic:', lm)
print('p-value', lm_pvalue)
print('F value:', fvalue)
print('F p-value', f_pvalue)
if lm_pvalue <= 0.05: print("Se rechaza H0")
else: print("No se rechaza H0")
```

```{python}
resid_poisson = model_poisson.resid_response
resid_nb = model_nb.resid_response

plt.scatter(model_nb.fittedvalues, resid_nb, label='NB')
plt.scatter(model_poisson.fittedvalues, resid_poisson, label='Poisson')
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.legend()
plt.show()
```

To compare the 2 models we will use the AIC (Akaike Information Criterion), which is a model selection criterion that seeks to balance the complexity of the model with its ability to fit the data. The AIC is calculated from the logarithm of the model's likelihood function and its complexity. The lower the AIC value, the better the model fits the data.

```{python}
# AIC y BIC del modelo Poisson
poisson_aic = model_poisson.aic
nb_aic = model_nb.aic
print("AIC Poisson:", poisson_aic)
print("AIC Binomial Negativo:", nb_aic)
```

Since the Poisson model has the lowest AIC, it is the model that best fits the data.

