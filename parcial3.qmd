---
title: Parcial 3 Santiago Uribe
jupyter: python3
---



```{python}
import pandas as pd
import statsmodels.formula.api as smf
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
```

```{python}
df = pd.read_csv('auto.csv')
dim_df = df.shape
print(f"The dataset consists of {dim_df[0]} rows and {dim_df[1]} columns")
```

Let’s check how much missing data there is by columns

```{python}
# Missing values by column
df.isnull().sum()
```

# Descriptive Statistics

```{python}
df.describe()
```

```{python}
#Separate attributes by type
int_attributes = df.select_dtypes(include=['int']).columns.to_list()
float_attributes = df.select_dtypes(include=['float']).columns.to_list()
```

```{python}
int_attributes
```

```{python}
float_attributes
```

With this we can see that the variables 'cylinders', 'horsepower', 'weight', 'year', 'origin' are discrete and the variables 'mpg', 'displacement', 'acceleration' are continuous.

### Histograms and Bar Diagrams

```{python}
fig, axs = plt.subplots(2, 4, figsize=(12,6))
axs = axs[:8].ravel()

for i, column in enumerate(df.columns):
    column_data = df[column]
    column_type = column_data.dtype
    if column_type == 'int64':  # Variables discretas
        sns.countplot(data=df, x=column, ax=axs[i])
        if column in ['horsepower', 'weight']:
            if column == 'horsepower': axs[i].set_xticks(axs[i].get_xticks()[::10])
            else: axs[i].set_xticks(axs[i].get_xticks()[::20])
            axs[i].tick_params(axis='x', rotation=90)

    elif column_type == 'float64':  # Variables continuas
        sns.histplot(data=df, x=column, kde=True, ax=axs[i])
        
plt.tight_layout()
plt.show()
```

### Boxplots

```{python}
numeric_attributes = df.select_dtypes(include=['float', 'int']).columns.to_list()
```

```{python}
import matplotlib.patches as mpatches

fliers = dict(markerfacecolor='m', marker='D') #atypical data
mean_ = dict(markerfacecolor='green', marker='D')
mean_artist = mpatches.Patch(facecolor='green', label='Mean')
ad_artist = mpatches.Patch(color='m', label='Atypical Data')

fig, axs = plt.subplots(2, 4, figsize=(12,6))
axs = axs[:8].ravel()
for i, col in enumerate(numeric_attributes):
  bp = axs[i].boxplot(df[col], vert=False, flierprops=fliers, showmeans=True, meanprops=mean_)
  axs[i].legend(handles=[mean_artist, ad_artist], loc='upper left', fontsize=8)
  axs[i].set_title(col)

plt.tight_layout()
plt.show()
```

The descriptive analysis performed reveals some interesting characteristics of the variables under study. It is observed that the variable "mpg" (miles per gallon) and "displacement" (engine displacement) show a rightward skewness in their distributions. This implies that there is a concentration of lower values at the left end of the distributions and a long tail towards higher values. On the other hand, the variable "acceleration" shows a distribution that could be considered centered, since the median and mean are approximately equal. In addition, the width of the box in the box plot indicates a low interquartile range.

For the variables "cylinders" (number of cylinders) and "origin" (origin of the car), it is observed that no whiskers are shown in the box plot. This indicates that the maximum value is equal to the third quartile and the minimum value is equal to the first quartile, respectively. In other words, there are no outliers in these variables and the distribution is completely contained within the box. You can see that most of the cars are American.

In the case of the variable "weight" (vehicle weight), there is a wide range of weights, with an almost uniform distribution. The absolute frequency shows that there are several different weights without repeating with a maximum frequency of 3.

Finally, the variable "horsepower" exhibits a high variability in the data. It is observed that the absolute frequency varies between approximately 2 and 10 for different horsepower values.

### Correlation

```{python}
fig, ax = plt.subplots(figsize=(10, 10))
corr_matrix = df.corr()
sns.heatmap(corr_matrix, square=True, annot=True, ax=ax)
plt.show()
```

In the correlation analysis, the following relationships stand out:

* The correlation between displacement and cylinders is 0.95. This strong positive correlation suggests that as engine displacement increases, the number of cylinders also increases. This makes sense, since larger engines tend to have more cylinders to generate more power.

* The correlation between displacement and horsepower is 0.9. This positive correlation also indicates a strong relationship, meaning that as engine displacement increases, horsepower tends to increase as well. This is logical, as larger engines tend to have higher horsepower.

* The correlation between horsepower and weight is 0.86. This positive correlation suggests that as engine horsepower increases, vehicle weight tends to increase as well. This could be attributed to the fact that more powerful vehicles often require heavier components to support that additional power.

* The correlation between horsepower and cylinders is 0.84. This positive correlation indicates that as engine horsepower increases, the number of cylinders tends to increase as well. This is consistent with the correlation observed between displacement and cylinders, since both attributes are related to engine size and power (to take into account for a multiple linear regression model).

* The correlation between mpg (miles per gallon) and weight is -0.83. This strong negative correlation indicates that as vehicle weight increases, fuel economy in terms of miles per gallon decreases. This is understandable, as heavier vehicles generally require more energy to move and therefore have higher fuel consumption.

* The correlation between mpg and displacement is -0.81. This negative correlation suggests that as engine displacement increases, fuel economy in terms of miles per gallon decreases. This makes sense, as larger and more powerful engines tend to consume more fuel.

Based on this, the following models are proposed

```{python}
# Support functions

def plot_regression_model(model, x_label, y_label):
    x = df[x_label]
    y = df[y_label]
    plt.scatter(x, y)
    plt.plot(x, model.predict(), color='red', linewidth=2)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.title(f'Simple Linear Regression: {x_label} vs {y_label}')
    plt.show()

from scipy.stats import shapiro
from statsmodels.stats.diagnostic import het_breuschpagan

def checkModelAssumptions(model):
    residuos = model.resid
    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 6))
    #QQ plot
    sm.qqplot(residuos, line='45', ax=axs[0,0])
    axs[0,0].set_title('Residuals Q-Q plot')
    #Residuals Histogram
    axs[0,1].hist(residuos, bins=30, edgecolor='black')
    axs[0,1].set_xlabel('Residuos')
    axs[0,1].set_ylabel('Frecuencia')
    axs[0,1].set_title('Residuals Histogram')
    #Reiduals Boxplot
    axs[1,0].boxplot(residuos)
    axs[1,0].set_ylabel('Residuals')
    axs[1,0].set_title('Residuals Boxplot')
    #Residuals
    axs[1,1].scatter(model.fittedvalues, model.resid_pearson)
    axs[1,1].plot([min(model.fittedvalues), max(model.fittedvalues)], [0, 0], 'k--', lw=2)
    axs[1,1].set_xlabel('Fitted Values')
    axs[1,1].set_ylabel('Residuals')
    axs[1,1].set_title(' Standarized Residuals')
    plt.tight_layout()
    plt.show()

    #Normality test
    print('================= Normality test =================')
    stat, p = shapiro(residuos)
    print('Estadística de prueba:', stat)
    print('Valor p:', p)
    if p <= 0.05: print("Se rechaza H0")
    else: print("No se rechaza H0")
    #Homoscedasticity test
    print('================= Homoscedasticity test =================')
    lm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(residuos, model.model.exog)
    print('Lagrange multiplier statistic:', lm)
    print('p-value', lm_pvalue)
    print('F value:', fvalue)
    print('F p-value', f_pvalue)
    if lm_pvalue <= 0.05: print("Se rechaza H0")
    else: print("No se rechaza H0")
```

## Simple Linear Regression

```{python}
# Model for "displacement" and "cylinders"
model1 = smf.ols(formula='cylinders ~ displacement', data=df).fit()
print(model1.summary())
```

The coefficient of determination (R-squared) is 0.904, which means that approximately 90.4% of the variability in the number of cylinders can be explained by engine displacement. The adjusted R-squared is also 0.904, indicating that the model is not over-fit and that the inclusion of the displacement variable is significant. The estimated coefficients of the model are 2.4587 for the intercept and 0.0155 for the displacement coefficient. Both coefficients are statistically significant (p-values close to zero). Therefore, on average, it is expected that for each unit increase in engine displacement, the number of cylinders will increase by approximately 0.0155. The 95% confidence intervals for the coefficients indicate that we can be 95% confident that the true values of the coefficients are within the intervals [2.348, 2.570] for the intercept and [0.015, 0.016] for "displacement"

```{python}
plot_regression_model(model1, 'displacement', 'cylinders')
```

```{python}
checkModelAssumptions(model1)
```

Model 1 shows violations in the assumptions of normality and homoscedasticity for the residuals. In addition, the residuals follow a non-random pattern. This suggests that the model does not fully capture the structure of the data.

```{python}
# Model for "displacement" and "horsepower"
model2 = smf.ols(formula='horsepower ~ displacement', data=df).fit()
print(model2.summary())
```

The analysis of the second regression model shows a strong relationship between engine displacement and horsepower. The coefficient of determination (R-squared) is 0.805, indicating that approximately 80.5% of the variability in horsepower can be explained by engine displacement. The adjusted R-squared is also 0.805, indicating that the model is not over-fit and that the inclusion of the displacement variable is significant.

The estimated coefficients of the model are 40.3061 for the intercept and 0.3300 for the displacement coefficient. Both coefficients are statistically significant (p-values close to zero). This means that, on average, for each unit increase in engine displacement, the horsepower is expected to increase by approximately 0.3300.

The 95% confidence intervals for the coefficients indicate that we can be 95% confident that the true values of the coefficients are within the intervals [36.738, 43.875] for the intercept and [0.314, 0.346] for "displacement".

```{python}
plot_regression_model(model2, 'displacement', 'horsepower')
```

```{python}
checkModelAssumptions(model2)
```

For model 2, violations of the assumptions of normality and homoscedasticity of the residuals are also observed. The normality analysis of the residuals shows that they do not follow a normal distribution, as evidenced by the p-value significantly less than 0.05 and in the QQ plot. This indicates that the model residuals do not conform to the normality assumption, which may affect the validity of statistical inferences.

```{python}
# Model for "horsepower" and "cylinders"
model3 = smf.ols(formula='cylinders ~ horsepower', data=df).fit()
print(model3.summary())
```

The analysis of the third regression model reveals a significant relationship between horsepower and the number of cylinders. The coefficient of determination (R-squared) is 0.711, indicating that approximately 71.1% of the variability in the number of cylinders can be explained by horsepower. The adjusted R-squared is also 0.710, suggesting that the model is not over-fit and that the inclusion of the horsepower variable is significant.

The estimated coefficients of the model are 1.5692 for the intercept and 0.0374 for the horsepower coefficient. Both coefficients are statistically significant, as their p-values are close to zero. This implies that, on average, for each unit increase in horsepower, the number of cylinders is expected to increase by approximately 0.0374.

The 95% confidence intervals for the coefficients are [1.305, 1.833] for the intercept and [0.035, 0.040] for "horsepower." This suggests that we can be 95% confident that the true values of the coefficients fall within these intervals.

```{python}
plot_regression_model(model3, 'horsepower', 'cylinders')
```

```{python}
checkModelAssumptions(model3)
```

Although the residuals of model 3 approximate a normal distribution, the presence of heteroscedasticity and the non-random pattern in the residuals suggest that there are aspects of the model that can be improved.

```{python}
# Model for "mpg" and "weight"
model4 = smf.ols(formula='weight ~ mpg', data=df).fit()
print(model4.summary())
```

The fourth regression model examines the relationship between the weight of the vehicle and its miles per gallon (mpg). The analysis reveals a significant negative relationship between these variables. The coefficient of determination (R-squared) is 0.693, indicating that approximately 69.3% of the variability in vehicle weight can be explained by mpg. The adjusted R-squared is also 0.692, suggesting that the inclusion of the mpg variable is significant and the model is not over-fit.

The estimated coefficients of the model are 5101.1136 for the intercept and -90.5714 for the mpg coefficient. Both coefficients are statistically significant, as their p-values are close to zero. This implies that, on average, for each unit increase in mpg, the weight of the vehicle is expected to decrease by approximately 90.5714 pounds.

The 95% confidence intervals for the coefficients are [4952.701, 5249.526] for the intercept and [-96.578, -84.565] for "mpg." This indicates that we can be 95% confident that the true values of the coefficients fall within these intervals.

```{python}
plot_regression_model(model4, 'mpg', 'weight')
```

```{python}
checkModelAssumptions(model4)
```

Model 4 meets the assumptions of normality and homoscedasticity in the residuals. However, the presence of a U-shaped pattern in the residuals indicates the need to consider a more complex model specification to adequately capture the nonlinear relationship between the variables.

## Multiple Linear Regression

Based on the correlations, it appears that displacement, cylinders, horsepower, and weight are all strongly correlated with each other and with the target variable, mpg (miles per gallon). These variables can be considered as potential predictors in a multiple linear regression model.

```{python}
model = smf.ols(formula='mpg ~ displacement + cylinders + horsepower + weight', data=df).fit()
print(model.summary())
```

The model as a whole is statistically significant, with an F-value of 234.2 and a very low p-value (6.18e-102), indicating that at least one of the predictor variables has a significant effect on the target variable.

The coefficient of determination (R-squared) of the model is 0.708, which means that approximately 70.8% of the variability in the variable "mpg" can be explained by the predictor variables included in the model. The adjusted coefficient of determination (adjusted R-squared) is 0.705, which takes into account the number of predictor variables and penalizes the inclusion of irrelevant variables.

Examining the regression coefficients for each predictor variable, we note that the coefficient for "displacement" is close to zero (0.0001) and is not statistically significant (p > 0.05). This suggests that there is no significant linear relationship between the variable "displacement" and the target variable "mpg".

The coefficient for "cylinders" is -0.3933, but it is also not statistically significant (p > 0.05). This indicates that the variable "cylinders" does not have a significant linear effect on the variable "mpg" when controlling for the other predictor variables.

On the other hand, the coefficient for "horsepower" is -0.0428 and is statistically significant (p < 0.05). This suggests that, holding the other variables constant, a one unit increase in the horsepower variable is associated on average with a 0.0428 unit decrease in the mpg variable.

Finally, the coefficient for "weight" is -0.0053 and is also statistically significant (p < 0.05). This indicates that, holding the other variables constant, a one unit increase in the "weight" variable is associated on average with a 0.0053 unit decrease in the "mpg" variable.

In summary, the multiple linear regression model suggests that the variables "displacement" and "cylinders" do not have a significant linear relationship with the variable "mpg". However, the variables "horsepower" and "weight" do have a significant effect on the target variable, with higher "horsepower" and "weight" being associated with a decrease in "mpg".

```{python}
checkModelAssumptions(model)
```

The multiple linear regression model used to predict the variable "mpg" does not meet the assumptions of normality of the residuals and homoscedasticity. In addition, a higher clustering of the residuals is observed on the right side compared to the left side. This skewness in the distribution of the residuals may indicate an additional violation of the linearity assumption or may be related to the non-normality of the residuals.

## PCA

```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# The variable 'name', which does not contribute anything to the model, is eliminated.
# The target variable is also eliminated
df_copy = df.copy()
df_copy = df_copy.drop(['name', 'mpg'], axis=1)

# Standardize variables. This will ensure that the variables contribute equally to the analysis.
scaler = StandardScaler()
data_scaled = scaler.fit_transform(df_copy)

pca = PCA()
pca.fit(data_scaled)
X_pca = pca.fit_transform(data_scaled)

# Obtain variance explained by each component
explained_variance = pca.explained_variance_ratio_

plt.plot(np.cumsum(explained_variance))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.grid()
plt.show()
```

The first 3 components are chosen since they already explain 90% of the variance of the data.

```{python}
import statsmodels.api as sm

X_pca_selected = sm.add_constant(X_pca[:, :3])
y = df['mpg']
model_pca = sm.OLS(y, X_pca_selected).fit()
print(model_pca.summary())
```

The multiple linear regression model using the three principal components shows a good fit to the data. The R-squared coefficient is 0.788, indicating that approximately 78.8% of the variability in the target variable (mpg) can be explained by the selected predictor variables. This suggests that the principal components capture a significant part of the information contained in the original variables.

When analyzing the estimated coefficients, it is observed that component 1 (x1) has a significant negative effect on the value of the target variable (mpg). This implies that as the value of component 1 increases, the value of mpg is expected to decrease. Similarly, component 2 (x2) also has a negative effect, although in a smaller magnitude. Component 3 (x3) shows an even greater impact on the target variable, also with a negative effect.

The statistical significance of the coefficients is confirmed by the associated p-values. All coefficients have p-values less than 0.05, indicating that they are statistically significant. This means that we can be confident in the relationship between the principal components and the target variable in the model.

In conclusion, the multiple linear regression model using the three principal components provides good explanatory power for the variability in the target variable (mpg). 

```{python}
checkModelAssumptions(model_pca)
```

The analysis of the multiple linear regression model reveals some violations of the fundamental assumptions. First, it is observed that the residuals do not follow a normal distribution, which may affect the validity of the statistical inferences made. In addition, evidence of heteroscedasticity is found in the residuals, indicating that the variance of the errors is not constant. This may have implications for the precision of the estimated coefficients and the conclusions of the model. In addition, a higher clustering is observed on the right-hand side of the residuals, suggesting the presence of systematic patterns or nonlinearities that are not being captured by the model.

